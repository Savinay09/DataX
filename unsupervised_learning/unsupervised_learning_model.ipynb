{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages:\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np \n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import pytz\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gensim\n",
    "\n",
    "import guidedlda\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for pre-processing and cleaning of the data\n",
    "\n",
    "# Loading the dataset as a dataframe (replace \"newdf.csv\" with name of your respective csv file):\n",
    "df = pd.read_csv(\"newdf.csv\")\n",
    "\n",
    "# Cleaning and processing the dataframe: \n",
    "\n",
    "# Renaming Headers:\n",
    "df = df.rename(columns={'Subject Line': 'subject' , 'Sender': 'from' , 'Date': 'date'})\n",
    "\n",
    "# Converting date to datetime format and validating: \n",
    "df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, errors='coerce', utc=True))\n",
    "df = df[df['date'].notna()]\n",
    "df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'date'], format=\"%Y%m%d:%H:%M:%S.%f\")\n",
    "time_pattern ='[0-9]{2}'\n",
    "df['time']=[x.strftime(\"%H:%M:%S\") for x in df.loc[:,'date']]\n",
    "df['hour']=[re.findall(time_pattern,x)[0] for x in df.loc[:,\"time\"]]\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "\n",
    "# Removing Export Error:\n",
    "def remove_subject(regex, data):\n",
    "    boolean_list= data.subject.str.match(regex)\n",
    "    keep_index=list(boolean_list.where(boolean_list==False).dropna().index)\n",
    "    data = data.loc[keep_index,:]\n",
    "    return data\n",
    "error_regex = '\\#ERROR\\!'\n",
    "without_e = remove_subject(error_regex, df)\n",
    "df = without_e\n",
    "UTF_pattern ='.*(UTF).*'\n",
    "utf_pattern ='.*(utf).*'\n",
    "df = remove_subject(UTF_pattern, df)\n",
    "df = remove_subject(utf_pattern, df)\n",
    "\n",
    "# Eliminating duplicate subject lines:\n",
    "df = df.drop_duplicates(subset=['subject'])\n",
    "\n",
    "# Accumulating links and subjects:\n",
    "df['links'] = [re.findall(r'^https?:\\/\\/.*[\\r\\n]*', i) for i in df['subject']]\n",
    "df['subject'] = df['subject'].replace(to_replace=r'^https?:\\/\\/.*[\\r\\n]*',value='',regex=True)\n",
    "\n",
    "# Removing user tags:\n",
    "df['subject'] = df['subject'].replace(to_replace=r'/(@\\S*)/',value='',regex=True)\n",
    "\n",
    "#Remove hashtags:\n",
    "df['subject'] = df['subject'].replace(to_replace=r'/(#\\S*)/',value='',regex=True)\n",
    "\n",
    "#Remove numbers:\n",
    "# df['subject'] = df['subject'].replace(to_replace=r'\\d+', value='', regex = True)\n",
    "\n",
    "# Determining polarity and subjectivity: \n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "df['polarity'] = df['subject'].apply(pol)\n",
    "df['subjectivity'] = df['subject'].apply(sub)\n",
    "\n",
    "# Display purposes: \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing by account for English stopwords and terms with no definition/emotional value\n",
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing words based on GloVe:\n",
    "tokenizer = Tokenizer(num_words=5000) \n",
    "tokenizer.fit_on_texts(df.subject.values)\n",
    "words_to_index = tokenizer.word_index\n",
    "\n",
    "# Padding sequences:\n",
    "sequences = tokenizer.texts_to_sequences(df.subject.values)\n",
    "X = pad_sequences(sequences, padding='post', maxlen=20)\n",
    "\n",
    "# Display purposes:\n",
    "print(X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using glove vectorizer: \n",
    "token_vectorizer = CountVectorizer(tokenizer, stop_words=stop_words, ngram_range=(1, 4)) \n",
    "X = token_vectorizer.fit_transform(df.subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data validation purposes:\n",
    "X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of subject line terms from vector:\n",
    "tf_feature_names = token_vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing words/phrases from generated dictionary:\n",
    "list(word2id.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GuidedLDA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting unique and key distinguishing words as seed words for guidedLDA seeding: \n",
    "\n",
    "enthusiasm = ['great', 'happy', 'big', 'cheer', 'cool', 'celebrate', 'bright', 'award', 'awesome', 'beautiful', \n",
    "              'beauty', 'best', 'birthday', 'boost', 'amaze', 'anniversary', 'celebration', 'delicious', 'enjoy',\n",
    "             'excite', 'favorite', 'fun', 'game', 'games', 'happy', 'huge', 'joy', 'love', 'massive', 'mega', 'open', \n",
    "              'opening','sale', 'save big', 'say hello', 'sale start', 'shop', 'shopping', 'super', 'super sale', \n",
    "              'spring','summer', 'summer sale', 'gift', 'awarded', 'grand', 'smile', 'vacation', 'awesome', \n",
    "              'gifts', 'spring', 'swag', 'rewards', 'magic', 'shipped', 'breakthrough', 'motivate', 'goals',\n",
    "             'inspiration', 'travel', 'confidence', 'cute', 'adore']\n",
    "\n",
    "urgency = ['miss', 'time', 'action', 'apply', 'attention', 'chance', 'act fast', 'almost', 'clock', 'close',\n",
    "          'countdown', 'date', 'day', 'deadline', 'due', 'end', 'ends', 'expire', 'expires', 'final', 'finish', 'go',\n",
    "          'hour', 'hr', 'hurry', 'last', 'late', 'left', 'must', 'one day', 'require', 'sale end','sale last', \n",
    "           'save','soon', 'still', 'still time', 'soon', 'speed', 'time', 'today','tomorrow', 'get', 'ahead', \n",
    "          'verify', 'gone', 'openings', 'waiting', 'now']\n",
    "\n",
    "surprise = ['surprise', 'alert', 'early', 'fast', 'faster', 'finally', 'flash', 'forget', 'gift',\n",
    "           'heard', 'invitation', 'invite', 'launch', 'look', 'brand new', 'sale', 'surprise', 'unlimited', \n",
    "            'unlimited access', 'upcoming', 'update', 'upgrade', 'drop', 'announcement', 'venture', 'invited',\n",
    "           'bang']\n",
    "\n",
    "trust = ['confirm', 'advice', 'ask', 'comfort', 'comfy', 'contact', 'cozy', 'control', 'daily', 'annual',\n",
    "        'everyone', 'everyday', 'expert', 'fact', 'family', 'home', 'information', 'info', 'instructor',\n",
    "        'law', 'local', 'match', 'message','secures','security','scholarship', 'school', 'science', \n",
    "         'scientist', 'stats', 'subscription', 'summary', 'support', 'technology', 'thank', 'community', 'reasons', \n",
    "        'true', 'recommend', 'data', 'control', 'understanding', 'science', 'popular', 'assistant', 'guide',\n",
    "        'well', 'mental', 'therapy', 'with']\n",
    "\n",
    "curiosity = ['new', 'try', 'already', 'awaits', 'brand new', 'challenge', 'click', 'complete', 'activate',\n",
    "            'different', 'easy', 'enter', 'explore', 'find', 'help', 'idea', 'important', 'inside', 'learn',\n",
    "            'listen', 'mystery', 'opportunity', 'sale item', 'see', 'see new', 'see new post', 'skill', 'start',\n",
    "            'start new', 'start today', 'qualify', 'surprising', 'chance', 'added', 'release', 'releases', 'update',\n",
    "            'updates', 'arrived', 'celeb', 'celebrity', 'affair', 'affordable', 'introduce', 'introduces', \n",
    "            'anomaly', 'hot', 'look']\n",
    "\n",
    "greed = ['free', 'save', 'almost', 'benefit', 'budget', 'claim', 'could win', 'beyond', 'certificate', 'demand',\n",
    "        'double', 'extra', 'future', 'limited', 'loss', 'lose', 'max', 'money', 'one get','one get one', 'saving',\n",
    "        'stock', 'win', 'winner', 'bonus', 'points', 'free', 'million', 'billion', 'skills', 'skill', 'tools', \n",
    "        'strategy', 'strategies']\n",
    "\n",
    "exclusivity = ['special', 'available', 'bonus', 'come back', 'choose', 'choice', 'deserve', 'exclusive', 'honor',\n",
    "              'individual', 'meet', 'member', 'mood', 'offer', 'perfect','personal', 'premiere', 'premium', \n",
    "               'recommend', 'recommendation', 'recommends', 'secret', 'special offer', 'specialization',  \n",
    "               'select', 'self', 'together', 'vip', 'welcome', 'join', 'reservation', 'waiver', 'appreciate', \n",
    "              'appreciation', 'first']\n",
    "\n",
    "# Ensuring all words from the riginal list are in the word2id list: \n",
    "enthusiasm = [x for x in enthusiasm if x in list(word2id.keys())]\n",
    "urgency = [x for x in urgency if x in list(word2id.keys())]\n",
    "surprise = [x for x in surprise if x in list(word2id.keys())]\n",
    "trust = [x for x in trust if x in list(word2id.keys())]\n",
    "curiosity = [x for x in curiosity if x in list(word2id.keys())]\n",
    "greed = [x for x in greed if x in list(word2id.keys())]\n",
    "exclusivity = [x for x in exclusivity if x in list(word2id.keys())]\n",
    "\n",
    "# Creating an array of topics (targeted marketing emotions) with all relevent words:\n",
    "seed_topic_list = [enthusiasm, urgency, surprise, trust, curiosity, exclusivity]\n",
    "topics = ['enthusiasm', 'urgency', 'surprise', 'trust', 'curiosity', 'greed', 'exclusivity', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiating guidedLDA Model\n",
    "model = guidedlda.GuidedLDA(n_topics=8, n_iter=100, random_state=7, refresh=10)\n",
    "seed_topics = {} \n",
    "for t_id, st in enumerate(seed_topic_list): \n",
    "    for word in st: \n",
    "        seed_topics[word2id[word]] = t_id \n",
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up dataframe to list the top n words for each emotion category\n",
    "n_top_words = 15\n",
    "topic_word = model.topic_word_\n",
    "df_top_words = pd.DataFrame()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    df_top_words[topics[i]] = topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create numeric matrix containing emotional intensities for each subject line\n",
    "int_values = model.transform(X)\n",
    "int_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for int_values\n",
    "topics = ['enthusiasm', 'urgency', 'surprise', 'trust', 'curiosity', 'greed', 'exclusivity', 'other']\n",
    "df_topic_int = pd.DataFrame(int_values, columns = topics)\n",
    "df_topic_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sum column to make sure all probabilities add up to 1\n",
    "dominant_topic_num = np.argmax(df_topic_int.values, axis=1)\n",
    "dominant_topic_lab = [topics[i] for i in dominant_topic_num]\n",
    "df_topic_int['dominant topic'] = dominant_topic_lab\n",
    "df_topic_int['sum'] = df_topic_int['greed'] + df_topic_int['other'] + df_topic_int['enthusiasm'] + df_topic_int['urgency'] + df_topic_int['surprise'] + df_topic_int['trust'] + df_topic_int['curiosity']  + df_topic_int['exclusivity'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values that weren't adding up to 1 added up to 0.08 so checking if any values do exist\n",
    "df_topic_int.loc[df_topic_int['sum'] == 0.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting subject lines into the dataframe \n",
    "subject_lines = df['subject'].values\n",
    "df_topic_int['subject lines'] = subject_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dislaying the results\n",
    "df_topic_int.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Topic-Keyword Matrix:\n",
    "df_topic_keywords = pd.DataFrame(model.components_)\n",
    "\n",
    "# Assigning Column and Index:\n",
    "df_topic_keywords.columns = token_vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topics\n",
    "\n",
    "# View:\n",
    "df_topic_keywords = df_topic_keywords.transpose()\n",
    "df_topic_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
